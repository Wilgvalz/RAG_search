{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dabfc338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['func', 'target', 'cwe', 'project', 'commit_id', 'hash', 'size', 'message'],\n",
       "    num_rows: 264393\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"diversevul_20230702.json\")\n",
    "\n",
    "train_valid = dataset[\"train\"].train_test_split(test_size=0.2, seed=0, train_indices_cache_file_name=\"train.indices\")\n",
    "train_data, valid_data = train_valid[\"train\"], train_valid[\"test\"]\n",
    "valid_test = valid_data.train_test_split(test_size=0.5, seed=0, train_indices_cache_file_name=\"valid.indices\", test_indices_cache_file_name=\"test.indices\")\n",
    "valid_data, test_data = valid_test[\"train\"], valid_test[\"test\"]\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": valid_data,\n",
    "    \"test\": test_data,\n",
    "})\n",
    "\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "340fbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "#lst_train = pd.DataFrame(dataset[\"train\"]).loc[(dataset['train']['target'] == [1])]['func']\n",
    "lst_train = df_train.loc[(df_train['target'] == 1)]\n",
    "lst_train_func = lst_train['func']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52fbc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_func = list(lst_train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13e12f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_func_new_zet = []\n",
    "lst_train_func_new_unzet = []\n",
    "lst_train_func_new = []\n",
    "i = 0\n",
    "while i <= 200:\n",
    "        lst_train_func_new_zet.append(list(lst_train_func)[i])\n",
    "        lst_train_func_new_zet.append('\\n<sep>\\n')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c52821",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ex_rag_vuln_223_pr.txt\", \"a\") as file:\n",
    "    file.writelines(lst_train_func_new_zet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4c4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from pickle import dump\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode = 'rt', encoding = 'latin-1')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n<sep>\\n')\n",
    "def sentence_lenghts(sentences):\n",
    "    lenghts = [len(s.split()) for s in sentences]\n",
    "    return min(lenghts), max(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e68b899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import unicodedata\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('latin-1')\n",
    "        line = line.split()\n",
    "        line = [word.lower() for word in line]\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8d3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# файл расширения pdf нужно сконвертировать из полученного файла txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50dd9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ex_rag_vuln_223_pr.pdf'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "for i in range(len(lst_train_func_new_zet)):\n",
    "    if i%2 == 0:\n",
    "        sentences.append(lst_train_func_new_zet[i])\n",
    "minlen, maxlen = sentence_lenghts(sentences)\n",
    "print('RAG...: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "cleanf = clean_lines(sentences)\n",
    "filename = 'ex_rag_vuln_223_pr.pkl'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(cleanf, outfile)\n",
    "outfile.close()\n",
    "print(filename, 'saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter \n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s'%filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a420558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = get_tokens(line)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "def trim_vocab(vocab, min_occurrence):\n",
    "    tokens = [k for k,c in vocab.items() if c >= 2]\n",
    "    return set(tokens)\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append('unk')\n",
    "        new_line = ' '.join(new_tokens)\n",
    "        new_lines.append(new_line)\n",
    "    return new_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557c7b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(data):\n",
    "    data = data.replace('\\r', '')\n",
    "    data = data.replace('\\t', ' ')\n",
    "    data = data.lower()\n",
    "    \n",
    "    re_fun = re.compile(r'([_.->\\w]+)\\s*\\([^)]*\\)')\n",
    "    re_var = re.compile(r'\\s*([_\\d\\w]*)(\\s*)=(\\s*)[^;]*')\n",
    "\n",
    "    var_idx = 0 \n",
    "    func_idx = 0\n",
    "    words = []\n",
    "    for line in data.split('\\n'):\n",
    "\n",
    "        s = re_var.search(line)\n",
    "        if s:\n",
    "            var_idx += 1\n",
    "            var_name = s.group(1)\n",
    "            line = re.compile(r'([_\\d\\w]+\\s*=\\s*)').sub('var%d = '% var_idx, line)\n",
    "\n",
    "        s = re_fun.search(line)\n",
    "        if s:\n",
    "            got_stmt = False\n",
    "            for st in ['if','for','while']:\n",
    "                if line.find(st) >=0:\n",
    "                    got_stmt = True\n",
    "                    break\n",
    "            if not got_stmt:\n",
    "                func_idx += 1\n",
    "                func = s.group(1)\n",
    "                line = line.replace('%s' % func, 'func%d' % func_idx)\n",
    "\n",
    "        chars = ['(', ')', '{', '}', '*', '/', '+', '-', '=', ';', ',']\n",
    "        for ch in chars:\n",
    "            line = line.replace(ch, ' %s ' % ch)\n",
    "\n",
    "        if line and len(line) >= 1:         \n",
    "            for w in line.split(' '):\n",
    "                w = w.strip()\n",
    "                if w:\n",
    "                    words.append(w) \n",
    "\n",
    "    return words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4135c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz \n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() \n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d833fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf_2(pdf_path: str) -> list[dict]: #!!!\n",
    "    with open('ex_rag_vuln_223_pr.txt', 'r') as file:\n",
    "        docs = file.read()\n",
    "    dc= docs.split('<sep>')\n",
    "    doc = fitz.open('ex_rag_vuln_223_pr.pdf') \n",
    "    p_and_t = []\n",
    "    a = 0\n",
    "    i = 0\n",
    "    for page_number, page in tqdm(enumerate(doc)): \n",
    "        while i <=402:\n",
    "            text = page.get_text()  \n",
    "            text = text_formatter(text)\n",
    "            p_and_t.append({\"page_number\": page_number,  \n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\"<sep>\")),\n",
    "                                \"page_token_count\": len(text) / 4,\n",
    "                                 \"text\": dc[i]})\n",
    "            i += 1 \n",
    "    return p_and_t\n",
    "pages_and_texts_2 = open_and_read_pdf_2(pdf_path='ex_rag_vuln_223.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c324d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ex_rag_vuln_223_pr.pkl'\n",
    "lines1 = load_clean_sentences(filename)\n",
    "vocab = to_vocab(lines1)\n",
    "print('rag Vocabulary: %d' % len(vocab))\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New rag Vocabulary: %d' % len(vocab))\n",
    "#lines1 = update_dataset(lines1, vocab)\n",
    "filename = 'rag_vocab_vuln_223_pr.pkl'\n",
    "save_clean_sentences(lines1, filename)\n",
    "print(len(lines1))\n",
    "for i in range(34,70): #example\n",
    "    print(\"line\",i,\":\",lines1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429850bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "a = 0\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    item[\"sentences\"] = str(item[\"text\"])\n",
    "    \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0caf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 100000000 #random number +-\n",
    "\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    \n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218adbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pages_and_chunks_2 = []\n",
    "\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks_2.append(chunk_dict)\n",
    "\n",
    "\n",
    "len(pages_and_chunks_2)\n",
    "df = pd.DataFrame(pages_and_chunks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec0419",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.to(\"cuda\") \n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cuda\") \n",
    "\n",
    "for item in tqdm(pages_and_chunks_2):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9c0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_2 = [item[\"sentence_chunk\"] for item in pages_and_chunks_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be430a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_embeddings_2 = embedding_model.encode(text_chunks_2,\n",
    "                                               batch_size=32, \n",
    "                                               convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fced97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_2)\n",
    "embeddings_df_save_path = \"rag_chunks_and_vuln_df_2_pr.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0002c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5488e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rag_chunks_and_vuln_df_2_pr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m text_chunks_and_embedding_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrag_chunks_and_vuln_df_2_pr.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m text_chunks_and_embedding_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m text_chunks_and_embedding_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mfromstring(x\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[]\u001b[39m\u001b[38;5;124m\"\u001b[39m), sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     13\u001b[0m pages_and_chunks \u001b[38;5;241m=\u001b[39m text_chunks_and_embedding_df\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rag_chunks_and_vuln_df_2_pr.csv'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"rag_chunks_and_vuln_df_2_pr.csv\")\n",
    "\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78def35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb0dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=200):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ce877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "    \n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top(query: str,\n",
    "            embeddings: torch.tensor,\n",
    "            pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "            n_resources_to_return: int=5):\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    \n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        \n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        \n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594e955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
