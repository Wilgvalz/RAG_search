{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc4ec27a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['func', 'target', 'cwe', 'project', 'commit_id', 'hash', 'size', 'message'],\n",
       "    num_rows: 264393\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"diversevul_20230702.json\")\n",
    "\n",
    "train_valid = dataset[\"train\"].train_test_split(test_size=0.2, seed=0, train_indices_cache_file_name=\"train.indices\")\n",
    "train_data, valid_data = train_valid[\"train\"], train_valid[\"test\"]\n",
    "valid_test = valid_data.train_test_split(test_size=0.5, seed=0, train_indices_cache_file_name=\"valid.indices\", test_indices_cache_file_name=\"test.indices\")\n",
    "valid_data, test_data = valid_test[\"train\"], valid_test[\"test\"]\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_data,\n",
    "    \"validation\": valid_data,\n",
    "    \"test\": test_data,\n",
    "})\n",
    "\n",
    "dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d68c7759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.DataFrame(dataset[\"train\"])\n",
    "#lst_train = pd.DataFrame(dataset[\"train\"]).loc[(dataset['train']['target'] == [1])]['func']\n",
    "lst_train = df_train.loc[(df_train['target'] == 1)]\n",
    "lst_train_func = lst_train['func']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95ec8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_func = list(lst_train_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27085165",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_train_func_new_zet = []\n",
    "lst_train_func_new_unzet = []\n",
    "lst_train_func_new = []\n",
    "i = 0\n",
    "while i <= 200:\n",
    "        lst_train_func_new_zet.append(list(lst_train_func)[i])\n",
    "        lst_train_func_new_zet.append('\\n<sep>\\n')\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de758f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ex_rag_vuln_223_pr.txt\", \"a\") as file:\n",
    "    file.writelines(lst_train_func_new_zet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63edcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from pickle import dump\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, mode = 'rt', encoding = 'latin-1')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n<sep>\\n')\n",
    "def sentence_lenghts(sentences):\n",
    "    lenghts = [len(s.split()) for s in sentences]\n",
    "    return min(lenghts), max(lenghts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af111e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string \n",
    "import unicodedata\n",
    "def clean_lines(lines):\n",
    "    cleaned = list()\n",
    "    \n",
    "    for line in lines:\n",
    "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('latin-1')\n",
    "        line = line.split()\n",
    "        line = [word.lower() for word in line]\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9af13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# файл расширения pdf нужно сконвертировать из полученного файла txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae84637",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ex_rag_vuln_223_pr.pdf'\n",
    "doc = load_doc(filename)\n",
    "sentences = to_sentences(doc)\n",
    "for i in range(len(lst_train_func_new_zet)):\n",
    "    if i%2 == 0:\n",
    "        sentences.append(lst_train_func_new_zet[i])\n",
    "minlen, maxlen = sentence_lenghts(sentences)\n",
    "print('RAG...: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "cleanf = clean_lines(sentences)\n",
    "filename = 'ex_rag_vuln_223_pr.pkl'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(cleanf, outfile)\n",
    "outfile.close()\n",
    "print(filename, 'saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53177df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter \n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s'%filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c496fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = get_tokens(line)\n",
    "        vocab.update(tokens)\n",
    "    return vocab\n",
    "def trim_vocab(vocab, min_occurrence):\n",
    "    tokens = [k for k,c in vocab.items() if c >= 2]\n",
    "    return set(tokens)\n",
    "def update_dataset(lines, vocab):\n",
    "    new_lines = list()\n",
    "    for line in lines:\n",
    "        new_tokens = list()\n",
    "        for token in line.split():\n",
    "            if token in vocab:\n",
    "                new_tokens.append(token)\n",
    "            else:\n",
    "                new_tokens.append('unk')\n",
    "        new_line = ' '.join(new_tokens)\n",
    "        new_lines.append(new_line)\n",
    "    return new_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54025fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(data):\n",
    "    data = data.replace('\\r', '')\n",
    "    data = data.replace('\\t', ' ')\n",
    "    data = data.lower()\n",
    "    \n",
    "    re_fun = re.compile(r'([_.->\\w]+)\\s*\\([^)]*\\)')\n",
    "    re_var = re.compile(r'\\s*([_\\d\\w]*)(\\s*)=(\\s*)[^;]*')\n",
    "\n",
    "    var_idx = 0 \n",
    "    func_idx = 0\n",
    "    words = []\n",
    "    for line in data.split('\\n'):\n",
    "\n",
    "        s = re_var.search(line)\n",
    "        if s:\n",
    "            var_idx += 1\n",
    "            var_name = s.group(1)\n",
    "            line = re.compile(r'([_\\d\\w]+\\s*=\\s*)').sub('var%d = '% var_idx, line)\n",
    "\n",
    "        s = re_fun.search(line)\n",
    "        if s:\n",
    "            got_stmt = False\n",
    "            for st in ['if','for','while']:\n",
    "                if line.find(st) >=0:\n",
    "                    got_stmt = True\n",
    "                    break\n",
    "            if not got_stmt:\n",
    "                func_idx += 1\n",
    "                func = s.group(1)\n",
    "                line = line.replace('%s' % func, 'func%d' % func_idx)\n",
    "\n",
    "        chars = ['(', ')', '{', '}', '*', '/', '+', '-', '=', ';', ',']\n",
    "        for ch in chars:\n",
    "            line = line.replace(ch, ' %s ' % ch)\n",
    "\n",
    "        if line and len(line) >= 1:         \n",
    "            for w in line.split(' '):\n",
    "                w = w.strip()\n",
    "                if w:\n",
    "                    words.append(w) \n",
    "\n",
    "    return words\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c14cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz \n",
    "from tqdm.auto import tqdm  \n",
    "\n",
    "def text_formatter(text: str) -> str:\n",
    "    \n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip() \n",
    "    \n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bf695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_read_pdf_2(pdf_path: str) -> list[dict]: #!!!\n",
    "    with open('ex_rag_vuln_223_pr.txt', 'r') as file:\n",
    "        docs = file.read()\n",
    "    dc= docs.split('<sep>')\n",
    "    doc = fitz.open('ex_rag_vuln_223_pr.pdf') \n",
    "    p_and_t = []\n",
    "    a = 0\n",
    "    i = 0\n",
    "    for page_number, page in tqdm(enumerate(doc)): \n",
    "        while i <=402:\n",
    "            text = page.get_text()  \n",
    "            text = text_formatter(text)\n",
    "            p_and_t.append({\"page_number\": page_number,  \n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\"<sep>\")),\n",
    "                                \"page_token_count\": len(text) / 4,\n",
    "                                 \"text\": dc[i]})\n",
    "            i += 1 \n",
    "    return p_and_t\n",
    "pages_and_texts_2 = open_and_read_pdf_2(pdf_path='ex_rag_vuln_223.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83763f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'ex_rag_vuln_223_pr.pkl'\n",
    "lines1 = load_clean_sentences(filename)\n",
    "vocab = to_vocab(lines1)\n",
    "print('rag Vocabulary: %d' % len(vocab))\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "print('New rag Vocabulary: %d' % len(vocab))\n",
    "#lines1 = update_dataset(lines1, vocab)\n",
    "filename = 'rag_vocab_vuln_223_pr.pkl'\n",
    "save_clean_sentences(lines1, filename)\n",
    "print(len(lines1))\n",
    "for i in range(34,70): #example\n",
    "    print(\"line\",i,\":\",lines1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc1992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29173e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "a = 0\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    item[\"sentences\"] = str(item[\"text\"])\n",
    "    \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261cc90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de45f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 100000000 #random number +-\n",
    "\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    \n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058eb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pages_and_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pages_and_chunks_2 = []\n",
    "\n",
    "for item in tqdm(pages_and_texts_2):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "\n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks_2.append(chunk_dict)\n",
    "\n",
    "\n",
    "len(pages_and_chunks_2)\n",
    "df = pd.DataFrame(pages_and_chunks_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898c778",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.to(\"cuda\") \n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=\"cuda\") \n",
    "\n",
    "for item in tqdm(pages_and_chunks_2):\n",
    "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_2 = [item[\"sentence_chunk\"] for item in pages_and_chunks_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609ad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk_embeddings_2 = embedding_model.encode(text_chunks_2,\n",
    "                                               batch_size=32, \n",
    "                                               convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7666d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_2)\n",
    "embeddings_df_save_path = \"rag_chunks_and_vuln_df_2_pr.csv\"\n",
    "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b7a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text_chunks_and_embedding_df = pd.read_csv(\"rag_chunks_and_vuln_df_2_pr.csv\")\n",
    "\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0907fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\", \n",
    "                                      device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029025e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=200):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027b507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \n",
    "    query_embedding = model.encode(query, \n",
    "                                   convert_to_tensor=True) \n",
    "    \n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "def print_top(query: str,\n",
    "            embeddings: torch.tensor,\n",
    "            pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "            n_resources_to_return: int=5):\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings,\n",
    "                                                  n_resources_to_return=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    \n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        \n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        \n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        \n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cdde7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
